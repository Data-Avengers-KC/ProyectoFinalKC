import json
import pathlib
from typing import List
import faiss  # type: ignore
from sentence_transformers import SentenceTransformer  # type: ignore

from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from secrets_ApiKey import get_openai_api_key


# ================== CONFIG ==================
BASE_DIR = pathlib.Path(__file__).resolve().parent

INDEX_PATHS = {
    "dmae":        BASE_DIR / "vectorstores" / "dmae_index.faiss",
    "catarata":    BASE_DIR / "vectorstores" / "catarata_index.faiss",
    "retinopatia": BASE_DIR / "vectorstores" / "retinopatia_index.faiss",
    "miopia":      BASE_DIR / "vectorstores" / "miopia_index.faiss",
}

EMB_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
EXPERIMENT_NAME = "RAG_Clinico"

OPENAI_MODEL = "gpt-4o-mini"
OPENAI_TEMPERATURE = 0.0

# Prompt orientado a recomendaciones cl√≠nicas
template = """
SISTEMA: Eres un asistente cl√≠nico especializado en OFTALMOLOG√çA (nivel experto). Responde siempre en espa√±ol. 
TU ROL: ayudar al profesional sanitario con recomendaciones basadas √∫nicamente en el CONTEXTO proporcionado; 
no hagas conjeturas fuera de ese contexto. No dispares consejos m√©dicos definitivos sin indicar nivel de incertidumbre. 
Siempre sugiere derivaci√≥n cuando haya signos de alarma.

INSTRUCCIONES:
1) Usa exclusivamente la informaci√≥n incluida en el campo "context".
2) Si falta informaci√≥n clave, enum√©rala en "Informaci√≥n faltante" y explica por qu√© es cr√≠tica.
3) Organiza la respuesta en las siguientes secciones:
   - Diagn√≥stico diferencial (con probabilidad, justificaci√≥n, nivel de evidencia, confianza).
   - Pruebas diagn√≥sticas recomendadas (con prioridad, hallazgos esperados, contraindicaciones, referencias).
   - Opciones de tratamiento (con indicaci√≥n, dosis si procede, contraindicaciones, nivel de evidencia).
   - Se√±ales de alarma / Red Flags (que requieren derivaci√≥n urgente).
   - Plan de seguimiento (cu√°ndo, qu√© monitorizar, pr√≥ximos pasos).
   - Comunicaci√≥n con el paciente (explicaci√≥n breve y clara en 2-4 frases).
   - Informaci√≥n faltante.
   - Nivel de confianza global.
   - Referencias.
4) Indica prioridad en URGENTE / ALTA / MEDIA / BAJA.
5) Usa vi√±etas o listas claras; no es necesario devolver JSON ni estructuras de programaci√≥n.
6) Recuerda: esto es apoyo cl√≠nico; siempre recomienda evaluaci√≥n presencial cuando sea apropiado.

FORMATO DE SALIDA ESPERADO:

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üîπ Diagn√≥stico diferencial
- Nombre: ‚Ä¶
- Probabilidad estimada: ‚Ä¶
- Razonamiento: ‚Ä¶
- Nivel de evidencia: ‚Ä¶
- Confianza: ‚Ä¶

üîπ Pruebas diagn√≥sticas recomendadas
- Prueba: ‚Ä¶
- Prioridad: ‚Ä¶
- Hallazgos esperados: ‚Ä¶
- Contraindicaciones: ‚Ä¶
- Referencias: ‚Ä¶

üîπ Opciones de tratamiento
- Opci√≥n: ‚Ä¶
- Indicaci√≥n: ‚Ä¶
- Dosis/R√©gimen: ‚Ä¶
- Contraindicaciones: ‚Ä¶
- Evidencia: ‚Ä¶
- Confianza: ‚Ä¶

Red Flags
- ‚Ä¶

Plan de seguimiento
- Cu√°ndo: ‚Ä¶
- Qu√© monitorizar: ‚Ä¶
- Pr√≥ximos pasos: ‚Ä¶

Comunicaci√≥n con el paciente
"‚Ä¶"

Informaci√≥n faltante
- ‚Ä¶

Nivel de confianza global
- ‚Ä¶

Referencias
- ‚Ä¶

Contexto:
{context}

Pregunta:
{question}

Responde con:
- Recomendaciones sobre nuevas pruebas diagn√≥sticas.
- Recomendaciones generales de tratamiento.
- Aspectos importantes a considerar para esta dolencia.
"""
prompt = PromptTemplate(
    input_variables=["context", "question"], template=template)


def load_faiss_index(index_path: pathlib.Path, model_name: str):
    """Carga √≠ndice FAISS y el modelo de embeddings usado al crearlo."""
    model = SentenceTransformer(model_name)
    index = faiss.read_index(str(index_path))
    return model, index


def retrieve_chunks(query: str, model: SentenceTransformer, index: faiss.Index, docs_jsonl: pathlib.Path, k: int = 6):
    """Busca los chunks m√°s relevantes para una query."""
    chunks = []
    with docs_jsonl.open("r", encoding="utf-8") as f:
        for line in f:
            chunks.append(json.loads(line))

    q_emb = model.encode([query], normalize_embeddings=True)
    D, I = index.search(q_emb, k)
    results = [chunks[i] for i in I[0] if 0 <= i < len(chunks)]
    return results


def generate_query_variants(base_query: str, n_variants: int = 3) -> List[str]:
    """Genera variantes de la query base usando LLM."""
    llm = ChatOpenAI(model_name=OPENAI_MODEL,
                     openai_api_key=get_openai_api_key(), temperature=0.0)
    variant_prompt = f"Genera {n_variants} variantes sem√°nticas de la siguiente consulta en espa√±ol, una por l√≠nea:\n\n{base_query}"
    resp = llm.predict(variant_prompt)
    variants = [v.strip("-‚Ä¢ \n") for v in resp.split("\n") if v.strip()]
    return [base_query] + variants


def reciprocal_rank_fusion(results_per_query: List[List[dict]], top_k: int = 6):
    """Fusi√≥n muy simple (dedup + top_k)."""
    seen = set()
    fused = []
    for docs in results_per_query:
        for d in docs:
            txt = d.get("text", "")
            if txt not in seen:
                seen.add(txt)
                fused.append(d)
    return fused[:top_k]


def run_rag_fusion(disease: str, edad: int, sexo: str, k: int = 6):

    # Caso de ojo normal
    if disease.lower() == "normal":
        answer = ("El an√°lisis indica que se trata de un ojo donde no se detectan nuestas patolog√≠as.")

        return answer

    # 1) Selecci√≥n del √≠ndice
    index_path = INDEX_PATHS.get(disease)

    if not index_path or not index_path.exists() or index_path == None:
        raise SystemExit(f"No existe √≠ndice para {disease} en {index_path}")

    jsonl_path = index_path.with_name(
        index_path.stem.replace("_index", "_chunks.jsonl"))

    # 2) Embeddings + FAISS
    model, index = load_faiss_index(index_path, EMB_MODEL_NAME)

    # 3) Construir query base
    base_query = (
        f"Paciente de {edad} a√±os, sexo {sexo}, con diagn√≥stico de {disease}. "
        f"¬øQu√© recomendaciones cl√≠nicas debo considerar?"
    )

    # 4) Variantes de query
    queries = generate_query_variants(base_query, n_variants=3)

    # 5) Recuperaci√≥n
    results_per_query = [retrieve_chunks(
        q, model, index, jsonl_path, k=k) for q in queries]

    # 6) Fusi√≥n
    fused_docs = reciprocal_rank_fusion(results_per_query, top_k=k)

    # 7) Contexto final
    context = "\n\n".join([d.get("text", "") for d in fused_docs])

    # 8) LLM respuesta
    llm = ChatOpenAI(model_name=OPENAI_MODEL, openai_api_key=get_openai_api_key(
    ), temperature=OPENAI_TEMPERATURE)
    final_prompt = prompt.format(context=context, question=base_query)
    answer = llm.predict(final_prompt)

    return answer


if __name__ == "__main__":

    run_rag_fusion("normal", edad=46, sexo="Mujer", k=6)
